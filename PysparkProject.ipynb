{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from sparkmeasure import StageMetrics\n",
    "\n",
    "#create a new Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('spark://con:7077') \\\n",
    "    .appName('PySpark Project') \\\n",
    "    .config(\"spark.drivers.cores\", \"2\") \\\n",
    "    .config(\"spark.drivers.memory\", \"4G\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#create spark metrics object\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f45577ad880>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.1.48:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://con:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySpark Project</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a csv file as a Spark DataFrame\n",
    "movie = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"data/movie.csv\") #filename to read from\n",
    "     )\n",
    "tag = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"data/tag.csv\") #filename to read from\n",
    "     )\n",
    "rating = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option('header', 'true') #means that the first line contains column names\n",
    "      .option(\"delimiter\", \",\") #set the delimiter to comma\n",
    "      .option(\"inferSchema\", \"true\") #automatically try to infer the column data types\n",
    "      .load(\"data/rating.csv\") #filename to read from\n",
    "     )\n",
    "\n",
    "# Register DataFrames as an SQL temporary view\n",
    "movie.createOrReplaceTempView(\"movie\")\n",
    "rating.createOrReplaceTempView(\"rating\")\n",
    "tag.createOrReplaceTempView(\"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22243\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 8\n",
      "elapsedTime => 8299 (8 s)\n",
      "stageDuration => 8146 (8 s)\n",
      "executorRunTime => 22025 (22 s)\n",
      "executorCpuTime => 17738 (18 s)\n",
      "executorDeserializeTime => 559 (0.6 s)\n",
      "executorDeserializeCpuTime => 141 (0.1 s)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 216 (0.2 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 57 (57 ms)\n",
      "resultSize => 12762 (12.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 192\n",
      "recordsRead => 20000264\n",
      "bytesRead => 692174705 (660.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 6\n",
      "shuffleTotalBlocksFetched => 6\n",
      "shuffleLocalBlocksFetched => 6\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 354 (354 Bytes)\n",
      "shuffleLocalBytesRead => 354 (354 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 354 (354 Bytes)\n",
      "shuffleRecordsWritten => 6\n"
     ]
    }
   ],
   "source": [
    "# 1st Query\n",
    "#start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "print(movie.join(rating, movie.movieId == rating.movieId, 'inner').filter(movie.title.contains('Jumanji')).count()\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------+\n",
      "|                               title|\n",
      "+------------------------------------+\n",
      "|         (500) Days of Summer (2009)|\n",
      "|101 Reykjavik (101 Reykjavík) (2000)|\n",
      "|             12 Years a Slave (2013)|\n",
      "|                         1408 (2007)|\n",
      "|   1492: Conquest of Paradise (1992)|\n",
      "+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 205\n",
      "elapsedTime => 6791 (7 s)\n",
      "stageDuration => 6481 (6 s)\n",
      "executorRunTime => 16002 (16 s)\n",
      "executorCpuTime => 4736 (5 s)\n",
      "executorDeserializeTime => 4331 (4 s)\n",
      "executorDeserializeCpuTime => 1390 (1 s)\n",
      "resultSerializationTime => 265 (0.3 s)\n",
      "jvmGCTime => 222 (0.2 s)\n",
      "shuffleFetchWaitTime => 1 (1 ms)\n",
      "shuffleWriteTime => 141 (0.1 s)\n",
      "resultSize => 1375754 (1343.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 1648885760\n",
      "recordsRead => 492842\n",
      "bytesRead => 23440100 (22.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 940\n",
      "shuffleTotalBlocksFetched => 523\n",
      "shuffleLocalBlocksFetched => 523\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 71541 (69.0 KB)\n",
      "shuffleLocalBytesRead => 71541 (69.0 KB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 71541 (69.0 KB)\n",
      "shuffleRecordsWritten => 940\n"
     ]
    }
   ],
   "source": [
    "# 2nd Query\n",
    "#start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "movie.join(tag, tag.movieId == movie.movieId, 'inner').filter(lower(tag.tag).contains('boring')).drop_duplicates(subset=['movieId']).orderBy(movie.title).select(movie.title).show(5, truncate=60)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "| 10573|\n",
      "| 19837|\n",
      "| 23333|\n",
      "| 25004|\n",
      "| 31338|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 210\n",
      "elapsedTime => 12833 (13 s)\n",
      "stageDuration => 12609 (13 s)\n",
      "executorRunTime => 33226 (33 s)\n",
      "executorCpuTime => 24343 (24 s)\n",
      "executorDeserializeTime => 3030 (3 s)\n",
      "executorDeserializeCpuTime => 947 (0.9 s)\n",
      "resultSerializationTime => 79 (79 ms)\n",
      "jvmGCTime => 813 (0.8 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 56 (56 ms)\n",
      "resultSize => 1326140 (1295.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 253755392\n",
      "recordsRead => 12661130\n",
      "bytesRead => 712627509 (679.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 24\n",
      "shuffleTotalBlocksFetched => 24\n",
      "shuffleLocalBlocksFetched => 24\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 1416 (1416 Bytes)\n",
      "shuffleLocalBytesRead => 1416 (1416 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 1416 (1416 Bytes)\n",
      "shuffleRecordsWritten => 24\n"
     ]
    }
   ],
   "source": [
    "# 3rd Query\n",
    "#start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"SELECT DISTINCT rating.userId FROM rating JOIN tag ON tag.userId = rating.userId AND tag.movieId = rating.movieId WHERE rating > 3 AND LOWER(tag) LIKE '%bollywood%' AND LOWER(tag) NOT LIKE '%not bollywood%' ORDER BY rating.userId\").show(5)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------+-------+\n",
      "|                                                       Title|Average|\n",
      "+------------------------------------------------------------+-------+\n",
      "|                                 Seven (a.k.a. Se7en) (1995)|    5.0|\n",
      "|Double Life of Veronique, The (Double Vie de Véronique, L...|    4.0|\n",
      "|                                 Fish Called Wanda, A (1988)|    3.0|\n",
      "|                                           Get Shorty (1995)|    3.0|\n",
      "+------------------------------------------------------------+-------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 207\n",
      "elapsedTime => 16667 (17 s)\n",
      "stageDuration => 16534 (17 s)\n",
      "executorRunTime => 42449 (42 s)\n",
      "executorCpuTime => 32571 (33 s)\n",
      "executorDeserializeTime => 2816 (3 s)\n",
      "executorDeserializeCpuTime => 834 (0.8 s)\n",
      "resultSerializationTime => 78 (78 ms)\n",
      "jvmGCTime => 1124 (1 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 8 (8 ms)\n",
      "resultSize => 1385535 (1353.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 46661632\n",
      "recordsRead => 20027541\n",
      "bytesRead => 692174705 (660.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 4\n",
      "shuffleTotalBlocksFetched => 4\n",
      "shuffleLocalBlocksFetched => 4\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 457 (457 Bytes)\n",
      "shuffleLocalBytesRead => 457 (457 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 457 (457 Bytes)\n",
      "shuffleRecordsWritten => 4\n"
     ]
    }
   ],
   "source": [
    "# 4th Query\n",
    "#start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"SELECT first(title) AS Title, AVG(rating) AS Average FROM movie INNER JOIN rating ON movie.movieId = rating.movieId WHERE YEAR(timestamp)=1995 GROUP BY rating.movieId ORDER BY Average DESC, Title ASC\").show(10, truncate=60)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------------------+------------------------------------------------------------+\n",
      "|                             title|                                                        Tags|\n",
      "+----------------------------------+------------------------------------------------------------+\n",
      "|\"\"Great Performances\"\" Cats (1998)|                                                        BD-R|\n",
      "|                'burbs, The (1989)|            1980's,black comedy,dark comedy,Joe Dante,quirky|\n",
      "|       (500) Days of Summer (2009)|intelligent,nonlinear,artistic,bittersweet,Funny,humor,hu...|\n",
      "| ...tick... tick... tick... (1970)|                                                        BD-R|\n",
      "|                          1 (2014)|                                                     Sukumar|\n",
      "+----------------------------------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 205\n",
      "elapsedTime => 4037 (4 s)\n",
      "stageDuration => 3904 (4 s)\n",
      "executorRunTime => 8788 (9 s)\n",
      "executorCpuTime => 3016 (3 s)\n",
      "executorDeserializeTime => 2093 (2 s)\n",
      "executorDeserializeCpuTime => 753 (0.8 s)\n",
      "resultSerializationTime => 99 (99 ms)\n",
      "jvmGCTime => 517 (0.5 s)\n",
      "shuffleFetchWaitTime => 16 (16 ms)\n",
      "shuffleWriteTime => 108 (0.1 s)\n",
      "resultSize => 1431897 (1398.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 11116832\n",
      "recordsRead => 492839\n",
      "bytesRead => 23440100 (22.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 4973\n",
      "shuffleTotalBlocksFetched => 773\n",
      "shuffleLocalBlocksFetched => 773\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 662937 (647.0 KB)\n",
      "shuffleLocalBytesRead => 662937 (647.0 KB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 662937 (647.0 KB)\n",
      "shuffleRecordsWritten => 4973\n"
     ]
    }
   ],
   "source": [
    "# 5th Query\n",
    "# #start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"SELECT title, concat_ws(',', collect_list(tag)) Tags FROM tag INNER JOIN movie ON tag.movieId = movie.movieId WHERE YEAR(timestamp) = 2015 GROUP BY tag.movieId, title ORDER BY title\").show(5, truncate=60)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------+-------------+\n",
      "|                           Title|Total_Ratings|\n",
      "+--------------------------------+-------------+\n",
      "|             Pulp Fiction (1994)|        67310|\n",
      "|             Forrest Gump (1994)|        66172|\n",
      "|Shawshank Redemption, The (1994)|        63366|\n",
      "|Silence of the Lambs, The (1991)|        63299|\n",
      "|            Jurassic Park (1993)|        59715|\n",
      "+--------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 207\n",
      "elapsedTime => 21385 (21 s)\n",
      "stageDuration => 21306 (21 s)\n",
      "executorRunTime => 63329 (1.1 min)\n",
      "executorCpuTime => 40118 (40 s)\n",
      "executorDeserializeTime => 1575 (2 s)\n",
      "executorDeserializeCpuTime => 713 (0.7 s)\n",
      "resultSerializationTime => 35 (35 ms)\n",
      "jvmGCTime => 7145 (7 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 151 (0.2 s)\n",
      "resultSize => 1352377 (1320.0 KB)\n",
      "diskBytesSpilled => 69445851 (66.0 MB)\n",
      "memoryBytesSpilled => 1174405120 (1120.0 MB)\n",
      "peakExecutionMemory => 1690828800\n",
      "recordsRead => 20027541\n",
      "bytesRead => 692174705 (660.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 113768\n",
      "shuffleTotalBlocksFetched => 1200\n",
      "shuffleLocalBlocksFetched => 1200\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 4693028 (4.0 MB)\n",
      "shuffleLocalBytesRead => 4693028 (4.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 4693028 (4.0 MB)\n",
      "shuffleRecordsWritten => 113768\n"
     ]
    }
   ],
   "source": [
    "# 6th Query\n",
    "# #start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"SELECT first(title) Title, count(rating) Total_Ratings FROM rating JOIN movie ON movie.movieId = rating.movieId GROUP BY rating.movieId ORDER BY Total_Ratings DESC\").show(5, truncate=60)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "source": [
    "# 7th Query\n",
    "# #start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"SELECT userId, count(*) Total_Ratings FROM rating WHERE YEAR(timestamp) = 1995 GROUP BY userId ORDER BY Total_Ratings DESC, userId ASC\").show(10)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------------+\n",
      "|userId|Total_Ratings|\n",
      "+------+-------------+\n",
      "|131160|            3|\n",
      "| 28507|            1|\n",
      "+------+-------------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 2\n",
      "numTasks => 206\n",
      "elapsedTime => 14798 (15 s)\n",
      "stageDuration => 14795 (15 s)\n",
      "executorRunTime => 40108 (40 s)\n",
      "executorCpuTime => 30233 (30 s)\n",
      "executorDeserializeTime => 2055 (2 s)\n",
      "executorDeserializeCpuTime => 721 (0.7 s)\n",
      "resultSerializationTime => 34 (34 ms)\n",
      "jvmGCTime => 1563 (2 s)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 7 (7 ms)\n",
      "resultSize => 1226634 (1197.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 69206016\n",
      "recordsRead => 20000263\n",
      "bytesRead => 690681057 (658.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 2\n",
      "shuffleTotalBlocksFetched => 2\n",
      "shuffleLocalBlocksFetched => 2\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 129 (129 Bytes)\n",
      "shuffleLocalBytesRead => 129 (129 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 129 (129 Bytes)\n",
      "shuffleRecordsWritten => 2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------------------+----------+\n",
      "|    Genre|               Title|Popularity|\n",
      "+---------+--------------------+----------+\n",
      "|   Action|Jurassic Park (1993)|     59715|\n",
      "|Adventure|Who Framed Roger ...|     21739|\n",
      "|Animation|Nightmare Before ...|     20509|\n",
      "| Children|E.T. the Extra-Te...|     32685|\n",
      "|   Comedy|As Good as It Get...|     21684|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 7\n",
      "numTasks => 925\n",
      "elapsedTime => 20876 (21 s)\n",
      "stageDuration => 20574 (21 s)\n",
      "executorRunTime => 52222 (52 s)\n",
      "executorCpuTime => 29660 (30 s)\n",
      "executorDeserializeTime => 7586 (8 s)\n",
      "executorDeserializeCpuTime => 3360 (3 s)\n",
      "resultSerializationTime => 174 (0.2 s)\n",
      "jvmGCTime => 949 (0.9 s)\n",
      "shuffleFetchWaitTime => 8 (8 ms)\n",
      "shuffleWriteTime => 4479 (4 s)\n",
      "resultSize => 2168498 (2.0 MB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 2376259200\n",
      "recordsRead => 20027541\n",
      "bytesRead => 692174705 (660.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 282592\n",
      "shuffleTotalBlocksFetched => 16762\n",
      "shuffleLocalBlocksFetched => 16762\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 6464027 (6.0 MB)\n",
      "shuffleLocalBytesRead => 6464027 (6.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 4997275 (4.0 MB)\n",
      "shuffleRecordsWritten => 168824\n"
     ]
    }
   ],
   "source": [
    "# 8th Query\n",
    "# #start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "# Find the genres\n",
    "genre = movie.select('title', 'movieId', split('genres', '[|]').alias('Genres')).select('title', 'movieId', explode('Genres')).where(\"col != '(no genres listed)'\")\n",
    "\n",
    "# Find most popular movies\n",
    "popular = spark.sql(\"SELECT movieId, COUNT(movieId) Popularity FROM rating GROUP BY movieId ORDER BY Popularity DESC\")\n",
    "\n",
    "popular.createOrReplaceTempView('popular')\n",
    "genre.createOrReplaceTempView('genre')\n",
    "\n",
    "# Group movies and order them by genre\n",
    "genre = spark.sql(\"SELECT title, movieId, first(col) Genre FROM genre GROUP BY title, movieId ORDER BY Genre\")\n",
    "genre.createOrReplaceTempView('genre')\n",
    "\n",
    "# Join Genres and Popular movies together\n",
    "popular = spark.sql(\"SELECT Genre, title, Popularity FROM popular INNER JOIN genre ON popular.movieId = genre.movieId ORDER BY Popularity DESC\")\n",
    "popular.createOrReplaceTempView('popular')\n",
    "\n",
    "# Group Genres and order by Genre ASC, Popularity DESC\n",
    "spark.sql(\"SELECT Genre, first(title) Title, first(Popularity) Popularity FROM popular GROUP BY Genre ORDER BY Genre, Popularity DESC\").show(5)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n",
      "|sum(Total)|\n",
      "+----------+\n",
      "|      4322|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "numTasks => 407\n",
      "elapsedTime => 61410 (1.0 min)\n",
      "stageDuration => 61386 (1.0 min)\n",
      "executorRunTime => 195708 (3.3 min)\n",
      "executorCpuTime => 129613 (2.2 min)\n",
      "executorDeserializeTime => 1777 (2 s)\n",
      "executorDeserializeCpuTime => 1155 (1 s)\n",
      "resultSerializationTime => 8 (8 ms)\n",
      "jvmGCTime => 8672 (9 s)\n",
      "shuffleFetchWaitTime => 7 (7 ms)\n",
      "shuffleWriteTime => 7177 (7 s)\n",
      "resultSize => 1109247 (1083.0 KB)\n",
      "diskBytesSpilled => 351896690 (335.0 MB)\n",
      "memoryBytesSpilled => 2215796094 (2.0 GB)\n",
      "peakExecutionMemory => 4613734400\n",
      "recordsRead => 20000263\n",
      "bytesRead => 690681057 (658.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 40000714\n",
      "shuffleTotalBlocksFetched => 41400\n",
      "shuffleLocalBlocksFetched => 41400\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 842786958 (803.0 MB)\n",
      "shuffleLocalBytesRead => 842786958 (803.0 MB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 842786958 (803.0 MB)\n",
      "shuffleRecordsWritten => 40000714\n"
     ]
    }
   ],
   "source": [
    "# 9th Query\n",
    "# #start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "rating.groupBy('movieId', 'timestamp').agg(countDistinct('userId').alias('Total')).filter('Total>1').select(sum('Total')).show()\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+-----+\n",
      "|      col|count|\n",
      "+---------+-----+\n",
      "|   Action|  212|\n",
      "|Adventure|  190|\n",
      "|Animation|   92|\n",
      "| Children|  113|\n",
      "|   Comedy|  728|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 4\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 6\n",
      "numTasks => 611\n",
      "elapsedTime => 17996 (18 s)\n",
      "stageDuration => 18554 (19 s)\n",
      "executorRunTime => 47558 (48 s)\n",
      "executorCpuTime => 28771 (29 s)\n",
      "executorDeserializeTime => 4224 (4 s)\n",
      "executorDeserializeCpuTime => 1938 (2 s)\n",
      "resultSerializationTime => 74 (74 ms)\n",
      "jvmGCTime => 1178 (1 s)\n",
      "shuffleFetchWaitTime => 19 (19 ms)\n",
      "shuffleWriteTime => 2196 (2 s)\n",
      "resultSize => 2081564 (2032.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 2446692800\n",
      "recordsRead => 10488252\n",
      "bytesRead => 714121157 (681.0 MB)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 89259\n",
      "shuffleTotalBlocksFetched => 3340\n",
      "shuffleLocalBlocksFetched => 3340\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 900605 (879.0 KB)\n",
      "shuffleLocalBytesRead => 900605 (879.0 KB)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 900605 (879.0 KB)\n",
      "shuffleRecordsWritten => 89259\n"
     ]
    }
   ],
   "source": [
    "# 10th Query\n",
    "# #start measuring performance\n",
    "stagemetrics.begin()\n",
    "\n",
    "# Find the genres\n",
    "genre = movie.select('title', 'movieId', split('genres', '[|]').alias('Genres')).select('title', 'movieId', explode('Genres')).where(\"col != '(no genres listed)'\")\n",
    "\n",
    "# Find movies with rating > 3.5 and tagged as funny\n",
    "funny = spark.sql(\"SELECT movieId FROM tag WHERE LOWER(tag) LIKE '%funny%' AND LOWER(tag) NOT LIKE '%not funny%' GROUP BY movieId\")\n",
    "\n",
    "rated = spark.sql(\"SELECT movieId FROM rating WHERE rating > 3.5 GROUP BY movieId\")\n",
    "\n",
    "funny.createOrReplaceTempView('funny')\n",
    "rated.createOrReplaceTempView('rated')\n",
    "\n",
    "fr = funny.join(rated, funny.movieId == rated.movieId, 'inner').select(tag.movieId)\n",
    "\n",
    "fr.join(genre, fr.movieId == genre.movieId, 'inner').groupby('col').count().sort('col').show(5)\n",
    "\n",
    "#stop measuring performance\n",
    "stagemetrics.end()\n",
    "#print performance metrics\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}